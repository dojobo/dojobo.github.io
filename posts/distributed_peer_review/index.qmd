---
title: "The ESO Distributed Peer Review Experiment"
author: "Dominic Bordelon"
date: "2019-09-01"
categories: [research software engineering, open science]
image: "dpr.png"
---

The [European Southern Observatory](https://www.eso.org/public/) builds, maintains, and operates ground-based telescopes. Their flagship facility since 1999 has been the [Very Large Telescope (VLT)](https://en.wikipedia.org/wiki/Very_Large_Telescope), which hosts four "unit telescope" platforms, each able to mount up to four instruments.

Given the finite instrumentation on-site and finite number of (unobstructed) nights in a year, telescope time is a precious resource. Allocating that time fairly and in ways that will hopefully progress science is therefore a great undertaking by the astronomy community every year, involving several days of meetings at one location.

Led by postdoctoral fellow Wolfgang Kerzendorf and with support from Dr. Nando Patat of the Observing Programmes Office, we piloted an experiment in online, anonymized peer reviews by applicants for telescope time. Applicants' proposals were compared to the corpus of peers' works, using an NLP (natural language processing) method, in order to dynamically match proposals to relevant reviewers.

::: {#fig-dpr}
![](dpr.png){fig-align="center"}

Study design from the *Nature Astronomy* paper.
:::

My roles in this project were

-   general project ideation and planning,
-   design, development, and deployment of the web app used for reviews.[^1]

[^1]: We used [CherryPy](https://docs.cherrypy.dev/en/latest/) and [SQLAlchemy](https://www.sqlalchemy.org/) with a LAM stack. GitHub repository for the reviews web app: <https://github.com/wkerzendorf/deepthought_opc>

We found that the process appeared to effective (compared to the "live" allocation process using the same proposals), and that participants felt that the process was fair. The findings were used, so far, by ESO to implement an actual distributed peer review process in 2023, with "[promising outcomes](https://doi.org/10.18727/0722-6691/5316)."

Here is the abstract for the paper which we published in *Nature Astronomy*:

> While ancient scientists often had patrons to fund their work, peer review of proposals for the allocation of resources is a foundation of modern science. A very common method is that proposals are evaluated by a small panel of experts (due to logistics and funding limitations) nominated by the grant-giving institutions. The expert panel process introduces several issues, most notably the following: (1) biases may be introduced in the selection of the panel and (2) experts have to read a very large number of proposals. Distributed peer review promises to alleviate several of the described problems by distributing the task of reviewing among the proposers. Each proposer is given a limited number of proposals to review and rank. We present the result of an experiment running a machine-learning-enhanced distributed peer-review process for allocation of telescope time at the European Southern Observatory. In this work, we show that the distributed peer review is statistically the same as a 'traditional' panel, that our machine-learning algorithm can predict expertise of reviewers with a high success rate, and that seniority and reviewer expertise have an influence on review quality. The general experience has been overwhelmingly praised by the participating community (using an anonymous feedback mechanism).

Read the full paper here: <https://doi.org/10.1038/s41550-020-1038-y> or <https://arxiv.org/abs/2004.04165> (preprint)
